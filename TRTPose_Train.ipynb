{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "4xKJzsQndnkn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip \"/content/drive/MyDrive/train2017.zip\" -d \"train2017\"\n",
        "!unzip \"/content/drive/MyDrive/val2017.zip\" -d \"val2017\"\n",
        "!unzip \"/content/drive/MyDrive/annotations_trainval2017.zip\" -d \"annotations\""
      ],
      "metadata": {
        "id": "ppyGk2mnpkM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os \n",
        "path=os.listdir('train2017/train2017')\n",
        "print(len(path))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJJFoWIbqkE_",
        "outputId": "75f95059-3706-4623-9e26-a91ec523ddd9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "118287\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XOgrBmKaEsT6"
      },
      "outputs": [],
      "source": [
        "!python3 -m pip install --upgrade setuptools pip\n",
        "!python3 -m pip install nvidia-pyindex\n",
        "#!python3 -m pip install --upgrade nvidia-tensorrt\n",
        "!python3 -m pip install nvidia-tensorrt==7.2.* --index-url https://pypi.ngc.nvidia.com"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PfasHsW6FPEk"
      },
      "outputs": [],
      "source": [
        "import tensorrt\n",
        "print(tensorrt.__version__)\n",
        "assert tensorrt.Builder(tensorrt.Logger())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/NVIDIA/apex.git"
      ],
      "metadata": {
        "id": "kaoRP3UIoLZl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd apex\n",
        "!pip install -v --disable-pip-version-check --no-cache-dir ./\n",
        "%cd .."
      ],
      "metadata": {
        "id": "0TwVG1jwojr1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8WYYc9rXGBcJ"
      },
      "outputs": [],
      "source": [
        "!sudo apt install libnvinfer* libnvinfer-dev\n",
        "!pip install torch2trt-unofficial\n",
        "#%cd ..\n",
        "!sudo pip3 install tqdm cython pycocotools\n",
        "!sudo apt-get install python3-matplotlib\n",
        "!git clone https://github.com/NVIDIA-AI-IOT/trt_pose\n",
        "%cd trt_pose \n",
        "!sudo apt-get install libprotobuf* protobuf-compiler ninja-build\n",
        "!sudo python3 setup.py install\n",
        "!python3 setup.py build_ext --inplace\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv ../annotations/annotations/ /content/trt_pose\n",
        "!mv ../val2017/val2017 /content/trt_pose\n",
        "!mv ../train2017/train2017 /content/trt_pose"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ukhfbh4TIhgi",
        "outputId": "b07d6a2e-d650-43de-a49f-77f339a80b4e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mv: cannot move '../val2017/val2017' to '/content/val2017': Directory not empty\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 tasks/human_pose/preprocess_coco_person.py annotations/person_keypoints_train2017.json annotations/person_keypoints_train2017_modified.json"
      ],
      "metadata": {
        "id": "mZo-j0f8fLbS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20dc9b06-9abf-429d-9ca1-756b29013218"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading...\n",
            "Preprocessing...\n",
            "Saving...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 tasks/human_pose/preprocess_coco_person.py annotations/person_keypoints_val2017.json annotations/person_keypoints_val2017_modified.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4F6p32czR7-u",
        "outputId": "bab63b61-ad00-4aea-8bdd-8804df2f804f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading...\n",
            "Preprocessing...\n",
            "Saving...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import subprocess\n",
        "import torch\n",
        "import torchvision\n",
        "import os\n",
        "import torch.optim\n",
        "import tqdm\n",
        "import apex.amp as amp\n",
        "import time\n",
        "import json\n",
        "import pprint\n",
        "import torch.nn.functional as F\n",
        "from trt_pose.coco import CocoDataset, CocoHumanPoseEval\n",
        "from trt_pose.models import MODELS\n",
        "\n",
        "OPTIMIZERS = {\n",
        "    'SGD': torch.optim.SGD,\n",
        "    'Adam': torch.optim.Adam\n",
        "}\n",
        "\n",
        "EPS = 1e-6\n",
        "\n",
        "def set_lr(optimizer, lr):\n",
        "    for p in optimizer.param_groups:\n",
        "        p['lr'] = lr\n",
        "        \n",
        "        \n",
        "def save_checkpoint(model, directory, epoch):\n",
        "    if not os.path.exists(directory):\n",
        "        os.mkdir(directory)\n",
        "    filename = os.path.join(directory, 'epoch_%d.pth' % epoch)\n",
        "    print('Saving checkpoint to %s' % filename)\n",
        "    torch.save(model.state_dict(), filename)\n",
        "\n",
        "    \n",
        "def write_log_entry(logfile, epoch, train_loss, test_loss):\n",
        "    with open(logfile, 'a+') as f:\n",
        "        logline = '%d, %f, %f' % (epoch, train_loss, test_loss)\n",
        "        print(logline)\n",
        "        f.write(logline + '\\n')\n",
        "        \n",
        "device = torch.device('cuda')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    \n",
        "    # parser = argparse.ArgumentParser()\n",
        "    # parser.add_argument('config')\n",
        "    # args = parser.parse_args()\n",
        "    \n",
        "    # print('Loading config %s' % args.config)\n",
        "    argsconfig='tasks/human_pose/experiments/resnet18_baseline_att_224x224_A.json'\n",
        "    with open(argsconfig, 'r') as f:\n",
        "        config = json.load(f)\n",
        "        pprint.pprint(config)\n",
        "        \n",
        "    logfile_path = argsconfig + '.log'\n",
        "    \n",
        "    checkpoint_dir = argsconfig + '.checkpoints'\n",
        "    if not os.path.exists(checkpoint_dir):\n",
        "        print('Creating checkpoint directory % s' % checkpoint_dir)\n",
        "        os.mkdir(checkpoint_dir)\n",
        "    \n",
        "        \n",
        "    # LOAD DATASETS\n",
        "    \n",
        "    train_dataset_kwargs = config[\"train_dataset\"]\n",
        "    train_dataset_kwargs['transforms'] = torchvision.transforms.Compose([\n",
        "            torchvision.transforms.ColorJitter(**config['color_jitter']),\n",
        "            torchvision.transforms.ToTensor(),\n",
        "            torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "    \n",
        "    test_dataset_kwargs = config[\"test_dataset\"]\n",
        "    test_dataset_kwargs['transforms'] = torchvision.transforms.Compose([\n",
        "            torchvision.transforms.ToTensor(),\n",
        "            torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "    \n",
        "    if 'evaluation' in config:\n",
        "        evaluator = CocoHumanPoseEval(**config['evaluation'])\n",
        "    \n",
        "    train_dataset = CocoDataset(**train_dataset_kwargs)\n",
        "    test_dataset = CocoDataset(**test_dataset_kwargs)\n",
        "    \n",
        "    part_type_counts = test_dataset.get_part_type_counts().float().cuda()\n",
        "    part_weight = 1.0 / part_type_counts\n",
        "    part_weight = part_weight / torch.sum(part_weight)\n",
        "    paf_type_counts = test_dataset.get_paf_type_counts().float().cuda()\n",
        "    paf_weight = 1.0 / paf_type_counts\n",
        "    paf_weight = paf_weight / torch.sum(paf_weight)\n",
        "    paf_weight /= 2.0\n",
        "    \n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        train_dataset,\n",
        "        **config[\"train_loader\"]\n",
        "    )\n",
        "    \n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        test_dataset,\n",
        "        **config[\"test_loader\"]\n",
        "    )\n",
        "    \n",
        "    model = MODELS[config['model']['name']](**config['model']['kwargs']).to(device)\n",
        "    \n",
        "    if \"initial_state_dict\" in config['model']:\n",
        "        print('Loading initial weights from %s' % config['model']['initial_state_dict'])\n",
        "        model.load_state_dict(torch.load(config['model']['initial_state_dict']))\n",
        "    \n",
        "    optimizer = OPTIMIZERS[config['optimizer']['name']](model.parameters(), **config['optimizer']['kwargs'])\n",
        "    model, optimizer = amp.initialize(model, optimizer, opt_level=\"O1\")\n",
        "    \n",
        "    if 'mask_unlabeled' in config and config['mask_unlabeled']:\n",
        "        print('Masking unlabeled annotations')\n",
        "        mask_unlabeled = True\n",
        "    else:\n",
        "        mask_unlabeled = False\n",
        "        \n",
        "    for epoch in range(config[\"epochs\"]):\n",
        "        \n",
        "        if str(epoch) in config['stdev_schedule']:\n",
        "            stdev = config['stdev_schedule'][str(epoch)]\n",
        "            print('Adjusting stdev to %f' % stdev)\n",
        "            train_dataset.stdev = stdev\n",
        "            test_dataset.stdev = stdev\n",
        "            \n",
        "        if str(epoch) in config['lr_schedule']:\n",
        "            new_lr = config['lr_schedule'][str(epoch)]\n",
        "            print('Adjusting learning rate to %f' % new_lr)\n",
        "            set_lr(optimizer, new_lr)\n",
        "        \n",
        "        if epoch % config['checkpoints']['interval'] == 0:\n",
        "            save_checkpoint(model, checkpoint_dir, epoch)\n",
        "        \n",
        "           \n",
        "        \n",
        "        train_loss = 0.0\n",
        "        model = model.train()\n",
        "        for image, cmap, paf, mask in tqdm.tqdm(iter(train_loader)):\n",
        "            image = image.to(device)\n",
        "            cmap = cmap.to(device)\n",
        "            paf = paf.to(device)\n",
        "            \n",
        "            if mask_unlabeled:\n",
        "                mask = mask.to(device).float()\n",
        "            else:\n",
        "                mask = torch.ones_like(mask).to(device).float()\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            cmap_out, paf_out = model(image)\n",
        "            \n",
        "            cmap_mse = torch.mean(mask * (cmap_out - cmap)**2)\n",
        "            paf_mse = torch.mean(mask * (paf_out - paf)**2)\n",
        "            \n",
        "            loss = cmap_mse + paf_mse\n",
        "            \n",
        "            with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
        "                scaled_loss.backward()\n",
        "#             loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += float(loss)\n",
        "            \n",
        "        train_loss /= len(train_loader)\n",
        "        \n",
        "        test_loss = 0.0\n",
        "        model = model.eval()\n",
        "        for image, cmap, paf, mask in tqdm.tqdm(iter(test_loader)):\n",
        "      \n",
        "            with torch.no_grad():\n",
        "                image = image.to(device)\n",
        "                cmap = cmap.to(device)\n",
        "                paf = paf.to(device)\n",
        "                mask = mask.to(device).float()\n",
        "\n",
        "                if mask_unlabeled:\n",
        "                    mask = mask.to(device).float()\n",
        "                else:\n",
        "                    mask = torch.ones_like(mask).to(device).float()\n",
        "                \n",
        "                cmap_out, paf_out = model(image)\n",
        "                \n",
        "                cmap_mse = torch.mean(mask * (cmap_out - cmap)**2)\n",
        "                paf_mse = torch.mean(mask * (paf_out - paf)**2)\n",
        "\n",
        "                loss = cmap_mse + paf_mse\n",
        "\n",
        "                test_loss += float(loss)\n",
        "        test_loss /= len(test_loader)\n",
        "        \n",
        "        write_log_entry(logfile_path, epoch, train_loss, test_loss)\n",
        "        \n",
        "        \n",
        "        if 'evaluation' in config:\n",
        "            evaluator.evaluate(model, train_dataset.topology)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "80937fe3c27341ee9ffe38879aeafee9",
            "2869deb0352041bca305cd5f39108ab6",
            "f7879e073e444b4d9eb30b8761e0b252",
            "8e76c40e96ee4733914aaf2253791256",
            "f17006edc7b048d89c27621786fb449c",
            "5d1689f7af3041eba6f783e3fc5a97bc",
            "b568efca16434a949cab881071a66200",
            "5e88ae20562547cdb003884de96cbade",
            "cf5a8aedc18a4b9dafeda65a89d2fb05",
            "05edc433efff449bb864562a14781b55",
            "1b89d261d0d74d5485ce05c26030979b"
          ]
        },
        "id": "fG-RloBo-E94",
        "outputId": "e71efe44-822c-4139-82d8-6f436015051a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'checkpoints': {'interval': 3},\n",
            " 'color_jitter': {'brightness': 0.05,\n",
            "                  'contrast': 0.05,\n",
            "                  'hue': 0.01,\n",
            "                  'saturation': 0.05},\n",
            " 'epochs': 250,\n",
            " 'lr_schedule': {'0': 0.001, '150': 1e-05, '75': 0.0001},\n",
            " 'model': {'kwargs': {'cmap_channels': 18,\n",
            "                      'num_upsample': 3,\n",
            "                      'paf_channels': 42,\n",
            "                      'upsample_channels': 256},\n",
            "           'name': 'resnet18_baseline_att'},\n",
            " 'optimizer': {'kwargs': {'lr': 0.001}, 'name': 'Adam'},\n",
            " 'stdev_schedule': {'0': 0.025},\n",
            " 'test_dataset': {'annotations_file': 'annotations/person_keypoints_val2017_modified.json',\n",
            "                  'category_name': 'person',\n",
            "                  'image_shape': [224, 224],\n",
            "                  'images_dir': 'val2017',\n",
            "                  'is_bmp': False,\n",
            "                  'random_angle': [-0.0, 0.0],\n",
            "                  'random_scale': [1.0, 1.0],\n",
            "                  'random_translate': [-0.0, 0.0],\n",
            "                  'stdev': 0.025,\n",
            "                  'target_shape': [56, 56]},\n",
            " 'test_loader': {'batch_size': 64,\n",
            "                 'num_workers': 8,\n",
            "                 'pin_memory': True,\n",
            "                 'shuffle': True},\n",
            " 'train_dataset': {'annotations_file': 'annotations/person_keypoints_train2017_modified.json',\n",
            "                   'category_name': 'person',\n",
            "                   'image_shape': [224, 224],\n",
            "                   'images_dir': 'train2017',\n",
            "                   'is_bmp': False,\n",
            "                   'random_angle': [-0.2, 0.2],\n",
            "                   'random_scale': [0.5, 2.0],\n",
            "                   'random_translate': [-0.2, 0.2],\n",
            "                   'stdev': 0.025,\n",
            "                   'target_shape': [56, 56]},\n",
            " 'train_loader': {'batch_size': 64,\n",
            "                  'num_workers': 8,\n",
            "                  'pin_memory': True,\n",
            "                  'shuffle': True}}\n",
            "Cachefile found.  Loading from cache file...\n",
            "Generating intermediate tensors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2693it [00:07, 346.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving to intermediate tensors to cache file...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/44.7M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "80937fe3c27341ee9ffe38879aeafee9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O1\n",
            "cast_model_type        : None\n",
            "patch_torch_functions  : True\n",
            "keep_batchnorm_fp32    : None\n",
            "master_weights         : None\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O1\n",
            "cast_model_type        : None\n",
            "patch_torch_functions  : True\n",
            "keep_batchnorm_fp32    : None\n",
            "master_weights         : None\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "Adjusting stdev to 0.025000\n",
            "Adjusting learning rate to 0.001000\n",
            "Saving checkpoint to tasks/human_pose/experiments/resnet18_baseline_att_224x224_A.json.checkpoints/epoch_0.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1002/1002 [17:25<00:00,  1.04s/it]\n",
            "100%|██████████| 43/43 [00:31<00:00,  1.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0, 0.003695, 0.003446\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 1002/1002 [17:27<00:00,  1.05s/it]\n",
            "100%|██████████| 43/43 [00:31<00:00,  1.38it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1, 0.002796, 0.003201\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 1002/1002 [17:18<00:00,  1.04s/it]\n",
            "100%|██████████| 43/43 [00:31<00:00,  1.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2, 0.002598, 0.003029\n",
            "Saving checkpoint to tasks/human_pose/experiments/resnet18_baseline_att_224x224_A.json.checkpoints/epoch_3.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1002/1002 [17:26<00:00,  1.04s/it]\n",
            "100%|██████████| 43/43 [00:31<00:00,  1.38it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3, 0.002454, 0.002777\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 1002/1002 [17:13<00:00,  1.03s/it]\n",
            "100%|██████████| 43/43 [00:30<00:00,  1.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4, 0.002357, 0.002708\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 1002/1002 [17:12<00:00,  1.03s/it]\n",
            "100%|██████████| 43/43 [00:30<00:00,  1.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5, 0.002279, 0.002657\n",
            "Saving checkpoint to tasks/human_pose/experiments/resnet18_baseline_att_224x224_A.json.checkpoints/epoch_6.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1002/1002 [17:13<00:00,  1.03s/it]\n",
            "100%|██████████| 43/43 [00:30<00:00,  1.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6, 0.002228, 0.002580\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 1002/1002 [17:13<00:00,  1.03s/it]\n",
            "100%|██████████| 43/43 [00:30<00:00,  1.40it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7, 0.002170, 0.002521\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 1002/1002 [17:15<00:00,  1.03s/it]\n",
            "100%|██████████| 43/43 [00:31<00:00,  1.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8, 0.002129, 0.002519\n",
            "Saving checkpoint to tasks/human_pose/experiments/resnet18_baseline_att_224x224_A.json.checkpoints/epoch_9.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1002/1002 [17:16<00:00,  1.03s/it]\n",
            "100%|██████████| 43/43 [00:30<00:00,  1.40it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9, 0.002097, 0.002486\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 1002/1002 [17:16<00:00,  1.03s/it]\n",
            "100%|██████████| 43/43 [00:30<00:00,  1.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10, 0.002065, 0.002522\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 47%|████▋     | 466/1002 [08:09<12:41,  1.42s/it]"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "TRTPose_Train.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "80937fe3c27341ee9ffe38879aeafee9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2869deb0352041bca305cd5f39108ab6",
              "IPY_MODEL_f7879e073e444b4d9eb30b8761e0b252",
              "IPY_MODEL_8e76c40e96ee4733914aaf2253791256"
            ],
            "layout": "IPY_MODEL_f17006edc7b048d89c27621786fb449c"
          }
        },
        "2869deb0352041bca305cd5f39108ab6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5d1689f7af3041eba6f783e3fc5a97bc",
            "placeholder": "​",
            "style": "IPY_MODEL_b568efca16434a949cab881071a66200",
            "value": "100%"
          }
        },
        "f7879e073e444b4d9eb30b8761e0b252": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5e88ae20562547cdb003884de96cbade",
            "max": 46830571,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cf5a8aedc18a4b9dafeda65a89d2fb05",
            "value": 46830571
          }
        },
        "8e76c40e96ee4733914aaf2253791256": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_05edc433efff449bb864562a14781b55",
            "placeholder": "​",
            "style": "IPY_MODEL_1b89d261d0d74d5485ce05c26030979b",
            "value": " 44.7M/44.7M [00:00&lt;00:00, 164MB/s]"
          }
        },
        "f17006edc7b048d89c27621786fb449c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5d1689f7af3041eba6f783e3fc5a97bc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b568efca16434a949cab881071a66200": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5e88ae20562547cdb003884de96cbade": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf5a8aedc18a4b9dafeda65a89d2fb05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "05edc433efff449bb864562a14781b55": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b89d261d0d74d5485ce05c26030979b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
